# üîÑ Configuration d'Airflow avec Docker

## üìã Vue d'Ensemble

Airflow est maintenant configur√© pour fonctionner avec Docker Compose. Cela simplifie grandement la configuration et l'utilisation.

---

## üöÄ Lancement Rapide (Docker)

### √âtape 1 : Cr√©er les dossiers n√©cessaires

```bash
cd "/Users/matthieudollfus/Documents/Master 2/MLOps/emmaloou-ML_Ops"
mkdir -p airflow/logs airflow/config
```

### √âtape 2 : Lancer tous les services

```bash
# Arr√™ter les services existants (si lanc√©s)
docker compose down

# Lancer tous les services (MinIO, MLflow, PostgreSQL, Redis, Airflow)
docker compose up -d

# V√©rifier que tous les services sont lanc√©s
docker compose ps
```

### √âtape 3 : V√©rifier les logs d'initialisation

```bash
# Voir les logs d'Airflow init (pour v√©rifier que l'initialisation est r√©ussie)
docker compose logs airflow-init

# Voir les logs du webserver
docker compose logs airflow-webserver

# Voir les logs du scheduler
docker compose logs airflow-scheduler
```

### √âtape 4 : Acc√©der √† l'interface Airflow

Ouvrez votre navigateur sur : **http://localhost:8080**

- **Username** : `admin`
- **Password** : `admin`

Les DAGs devraient appara√Ætre automatiquement :
- `data_ingestion` : Ingestion des donn√©es vers MinIO
- `training` : Entra√Ænement du mod√®le

---

## üì¶ Services Docker

| Service | Port | URL | Description |
|---------|------|-----|-------------|
| **Airflow Webserver** | 8080 | http://localhost:8080 | Interface web Airflow |
| **Airflow Scheduler** | - | - | Planificateur des t√¢ches |
| **MinIO** | 9001 | http://localhost:9001 | Storage S3-compatible |
| **MLflow** | 5001 | http://localhost:5001 | Tracking ML |
| **PostgreSQL** | 5433 | localhost:5433 | Base de donn√©es Airflow |
| **Redis** | 6379 | localhost:6379 | Cache Airflow |

---

## üîß Installation des D√©pendances Python dans Airflow

Les scripts Airflow n√©cessitent PyTorch et d'autres d√©pendances. Pour les installer dans les containers :

### Option 1 : Installation manuelle (si n√©cessaire)

```bash
# Se connecter au container Airflow
docker compose exec airflow-webserver bash

# Installer les d√©pendances
pip install torch torchvision --index-url https://download.pytorch.org/whl/cpu
pip install boto3 mlflow pandas numpy Pillow scikit-learn requests

# Ou utiliser le script
bash /opt/airflow/docker/install_airflow_dependencies.sh
```

### Option 2 : Utiliser le Dockerfile personnalis√© (recommand√©)

Pour installer automatiquement les d√©pendances, vous pouvez utiliser le Dockerfile personnalis√© :

```bash
# Construire l'image personnalis√©e
docker build -f docker/Dockerfile.airflow -t mlops-airflow:custom .

# Puis modifier docker-compose.yml pour utiliser cette image
# Remplacer: image: apache/airflow:2.8.0
# Par: image: mlops-airflow:custom
```

---

## üìÅ Structure des Volumes

Les dossiers suivants sont mont√©s dans les containers Airflow :

```
./dags              ‚Üí /opt/airflow/dags       (DAGs Airflow)
./airflow/logs      ‚Üí /opt/airflow/logs      (Logs des t√¢ches)
./airflow/config    ‚Üí /opt/airflow/config    (Configuration)
./scripts           ‚Üí /opt/airflow/scripts   (Scripts Python)
./models            ‚Üí /opt/airflow/models     (Mod√®les entra√Æn√©s)
./data              ‚Üí /opt/airflow/data       (Donn√©es d'entra√Ænement)
```

---

## üß™ Tester les DAGs

### Dans l'interface Airflow

1. Allez sur http://localhost:8080
2. Connectez-vous (admin/admin)
3. Vous devriez voir 2 DAGs :
   - `data_ingestion`
   - `training`
4. Cliquez sur le bouton **‚ñ∂Ô∏è** pour d√©clencher un DAG manuellement
5. Cliquez sur le nom du DAG pour voir les d√©tails
6. Cliquez sur une t√¢che pour voir les logs

### Via la ligne de commande

```bash
# Lister les DAGs
docker compose exec airflow-webserver airflow dags list

# D√©clencher un DAG manuellement
docker compose exec airflow-webserver airflow dags trigger data_ingestion

# Voir les logs d'une t√¢che
docker compose exec airflow-webserver airflow tasks logs data_ingestion scan_images 2024-01-01
```

---

## üõ†Ô∏è Commandes Utiles

### Gestion des services

```bash
# D√©marrer les services
docker compose up -d

# Arr√™ter les services
docker compose down

# Red√©marrer un service sp√©cifique
docker compose restart airflow-webserver

# Voir les logs en temps r√©el
docker compose logs -f airflow-webserver
docker compose logs -f airflow-scheduler
```

### Gestion d'Airflow

```bash
# Se connecter au container
docker compose exec airflow-webserver bash

# Cr√©er un nouvel utilisateur (depuis le container)
airflow users create \
    --username user \
    --firstname User \
    --lastname User \
    --role User \
    --email user@example.com \
    --password password

# Lister les connexions
docker compose exec airflow-webserver airflow connections list
```

---

## ‚ö†Ô∏è Configuration des Connexions

Si vous devez configurer des connexions (MinIO, etc.) dans Airflow :

1. Allez dans **Admin ‚Üí Connections**
2. Ajoutez une nouvelle connexion :
   - **Connection Type** : S3
   - **Host** : `http://minio:9000`
   - **Login** : `minioadmin`
   - **Password** : `minioadmin`

Ou via la CLI :

```bash
docker compose exec airflow-webserver airflow connections add minio \
    --conn-type s3 \
    --conn-host http://minio:9000 \
    --conn-login minioadmin \
    --conn-password minioadmin
```

---

## üîç D√©pannage

### Les DAGs n'apparaissent pas

1. V√©rifiez les logs : `docker compose logs airflow-scheduler`
2. V√©rifiez que les DAGs sont dans `./dags/`
3. V√©rifiez les erreurs Python dans les logs
4. Red√©marrez le scheduler : `docker compose restart airflow-scheduler`

### Erreur "Module not found"

Les d√©pendances Python ne sont pas install√©es. Installez-les :

```bash
docker compose exec airflow-webserver pip install torch torchvision boto3 mlflow
```

### Erreur de connexion PostgreSQL

V√©rifiez que PostgreSQL est d√©marr√© :

```bash
docker compose ps postgres
docker compose logs postgres
```

### R√©initialiser compl√®tement Airflow

```bash
# Arr√™ter et supprimer les volumes
docker compose down -v

# Supprimer les dossiers Airflow locaux (ATTENTION : supprime les logs)
rm -rf airflow/logs/* airflow/config/*

# Relancer
docker compose up -d
```

---

## üìù Configuration Alternative (Local - Sans Docker)

Si vous pr√©f√©rez utiliser Airflow localement sans Docker :

### Initialiser la DB

```bash
cd "/Users/matthieudollfus/Documents/Master 2/MLOps/emmaloou-ML_Ops"
export AIRFLOW_HOME=$(pwd)/airflow
source venv/bin/activate

# Initialiser la base de donn√©es
airflow db init

# Cr√©er un utilisateur admin
airflow users create \
    --username admin \
    --firstname Admin \
    --lastname Admin \
    --role Admin \
    --email admin@example.com \
    --password admin
```

### Lancer Airflow (2 terminaux)

**Terminal 1 - Scheduler** :
```bash
export AIRFLOW_HOME=$(pwd)/airflow
source venv/bin/activate
airflow scheduler
```

**Terminal 2 - Webserver** :
```bash
export AIRFLOW_HOME=$(pwd)/airflow
source venv/bin/activate
airflow webserver --port 8080
```

---

## üéØ Prochaines √âtapes

1. ‚úÖ **Airflow configur√©** avec Docker
2. ‚úÖ **DAGs cr√©√©s** et pr√™ts √† √™tre utilis√©s
3. ‚û°Ô∏è **Tester les DAGs** manuellement dans l'interface
4. ‚û°Ô∏è **Configurer les connexions** MinIO si n√©cessaire
5. ‚û°Ô∏è **Automatiser** l'entra√Ænement avec le DAG training

---

**üéâ Airflow est maintenant pr√™t √† √™tre utilis√© !**
